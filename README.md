# Capstone: A Comparative Study of Monosemanticity Across Language Models

This project investigates how innate monosemantic features are distributed across different language model architectures, specifically comparing Google's Gemma, Meta's LLaMA, and OpenAI's GPT-2 models. The research builds upon recent work in neural network interpretability and feature extraction.

## Project Overview

We aim to understand how different model architectures and sizes influence the development of monosemantic features - neural network components that consistently track specific, interpretable concepts.

## Installation & Setup

```bash
pip install -r requirements.txt
```
## Usage

[TO DO]

## Contributors

- Einstein Oyewole (eo2233@nyu.edu)
- Alon Florentin (abf386@nyu.edu)

## References

1. Henighan et al. (2023). "Superposition, memorization, and double descent"
2. Templeton et al. (2024). "Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet"
3. Bricken et al. (2023). "Towards monosemanticity: Decomposing language models with dictionary learning"